

# WARNING: activate the conda environment on the workers before running the agent

# Wandb sweep configuration for coarse hyperparameter tuning.
# The search method is Bayesian and we only use train/val split, i.e. no KFoldCV.

# ====== Sweep-specific configuration
program: run_sweep.py       # script to run
method: bayes                      # method to go over hyperparams
description: "new WSS hyperparameter tuning"
metric:                           # reference metric, must be logged at top level
  name: val_f1_score
  goal: maximize

parameters:
  # ====== Model configuration, see config.yaml

  # --- Model
  model.type:
    value: "Equiv"
  model.name:
    value: "Equiv_GIN_WSS_KNN5"
  model.desc:
    value: "base model"

  # --- Dataset
  dataset.name:
    value: "WSSToCnc_KNN5"
  dataset.in_memory:
    value: true
  dataset.num_node_features:
    value: 30
  dataset.num_graph_features:
    value: 3

  # --- Cross val
  cv.valid_ratio:
    value: 0.25     # not applicable for kfold
  cv.k_fold:
    value: None
  cv.fold_id:
    value: None
  cv.seed:
    value: 0

  # ====== Actual hyperparameters

  # --- Architecture
  num_gin:
    values:
      - 0
      - 2

  num_equiv:
    values:
      - 3
      - 5
      #- 4
      #- 6

  num_hidden_dim:
    values:
      - 8
      - 12
      - 16

  # --- Optimizer
  optimizer.name:
    value: "Adam"

  optimizer.lr:
    distribution: log_uniform
    min: -12.206072645530174
    max: -5.298317366548036

  optimizer.momentum:
    value: 0.0

  # --- Loss
  loss.weight:
    distribution: uniform
    min: 0.2
    max: 0.8

  # --- Training
  batch_size:
    value: 12
  epochs:
    distribution: int_uniform
    min: 50
    max: 400

  early_stop:
    value: 100

  allow_stop:
    value: 100

  # ---
  physics:
    value: 0

